{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47670 Assignment 2\n",
    "## Robert Keenan 15333066\n",
    "## Date: 27/04/20\n",
    "\n",
    "The aim of this assignment was to scrape a number of customer reviews from a set of websites and to evaluate the performance of a number of text classification algorithms on the data. \n",
    "\n",
    "Firstly, I needed to choose three review categories to scrape the data from on the website http://mlg.ucd.ie/modules/yalp/ and then to store them as 3 separate datasets. For each review stored, the review text and a class label of positive or negative was stored also. \n",
    "\n",
    "The next steps were to run different classification algorithms on the data and compare their performance.\n",
    "\n",
    "## Review Categories\n",
    "The first step was to choose the review categories to complete the assignment on. In this case, I chose the following:\n",
    "\n",
    "- Automotive Category (132 businesses)\n",
    "- Gym Category (122 businesses)\n",
    "- Hotel Category (113 businesses)\n",
    "\n",
    "From here, I needed to set up my program to be able to scrape all of the reviews from each of these datasets and store in appropriate formats. \n",
    "\n",
    "\n",
    "There are 2 steps to be done first. I need to take the categories that are wanted and scrape their links down. \n",
    "Once those have been established, I need to scrape the individual businesses down for each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Rob/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import nltk\n",
    "link = \"http://mlg.ucd.ie/modules/yalp\"\n",
    "\n",
    "\n",
    "######\n",
    "##### FIXME do all of this work manually first and show the corrector that I know what I'm doing and then show \n",
    "##### the functions that I built\n",
    "def get_url_categories(link, categories_wanted):\n",
    "    category_links=[]\n",
    "    review_links = []\n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #Go through the links\n",
    "    for url in soup.find_all('a'):\n",
    "        current_link = url.get('href')\n",
    "        if current_link == 'index.html':\n",
    "            continue\n",
    "        if categories_wanted != None:\n",
    "            if any(category in current_link for category in categories_wanted):\n",
    "                category_links.append('/'+current_link)\n",
    "        #The review links are being done\n",
    "        else:\n",
    "            review_links.append('/'+current_link)\n",
    "    if categories_wanted != None: \n",
    "        return category_links\n",
    "    else:\n",
    "        return review_links\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#Now I need to take the reviews out\n",
    "\n",
    "def get_reviews_store(link, category):    \n",
    "    class_labels = []\n",
    "    review_text_sections = []\n",
    "    #Companies\n",
    "    #This will return a list of companies\n",
    "    companies = get_url_categories(link+category, None)\n",
    "    \n",
    "    for company in companies:\n",
    "        #For each company, I need to open the data of the \n",
    "        response = requests.get(link+company)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        #I have now opened the page for the individual company, and now need to look at \n",
    "        \n",
    "        #For each review in the list, I need to take out the text and the score\n",
    "        #If we look at the insepct element, we can see that the class review is used for any reviews in the space\n",
    "        for review in soup.find_all('div', class_ = 'review'):\n",
    "            #I need the rating at the \n",
    "            #Once I'm here I can take the review number and the review text itself\n",
    "            stars = review.find('img')['alt']\n",
    "            #The star is produced like '1-star' so i need to split on the - and take the number\n",
    "            num_stars = int(stars.split('-')[0])\n",
    "            #Negative review\n",
    "            if num_stars < 4:\n",
    "                class_label = -1\n",
    "            #Positive review\n",
    "            else:\n",
    "                class_label = 1\n",
    "            class_labels.append(class_label)\n",
    "            #I need to pull the text out of it now\n",
    "            review_text = review.find('p', class_ = 'review-text')\n",
    "            review_text = review_text.get_text(strip=True)\n",
    "            review_text_sections.append(review_text)\n",
    "    reviews_of_category = (class_labels, review_text_sections)\n",
    "    return reviews_of_category\n",
    "        \n",
    "\n",
    "categories = get_url_categories(link, ['automotive', 'gym', 'hotel'])\n",
    "automotive_reviews = get_reviews_store(link, categories[0])\n",
    "gym_reviews = get_reviews_store(link, categories[1])\n",
    "hotel_reviews = get_reviews_store(link, categories[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have stored the data in 3 separate datasets. They are stored in the form of a tuple with 2 lists for the class labels and the text. I have decided that anything that has less than 4 stars (1,2,3) is a negative review which is stored with a class label of -1 and anything that has 4 or 5 stars is classed as being a positive review and given a class label of 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 \n",
    "In terms of the next section, I need to take the 2 datasets, apply suitable preprocessing techniques to create a numeric representation of the data for each review in a category which will allow it to be suitably classified. \n",
    "\n",
    "Following this, I can build a classification model using a classifier and then test with this classifier to obtain classification results using an evaluation method. \n",
    "\n",
    "As we know, the text of the reviews is not numeric but is rather, text. As a result, it is needed to tokenize the text where each token corresponds to an individual word in the review. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "See Tfidf is better than Count because it takes into account how many documents a term appears in. Like if car turned up 100 times in one review, and service turned up 10 times in 10 reviews, service would rank higher in terms of importance than car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 1546 distinct terms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords = [stemmer.stem(word) for word in stopWords]\n",
    "\n",
    "#As seen from the 19 Introduction to Text Mining\n",
    "def lemma_tokenizer(text):\n",
    "    # use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(text)\n",
    "    # then use NLTK to perform lemmatisation on each token\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma_tokens.append( lemmatizer.lemmatize(token) )\n",
    "    return lemma_tokens\n",
    "\n",
    "automotive_targets  = automotive_reviews[0]\n",
    "automotive_data = automotive_reviews[1]\n",
    "#It removes the stop words, the vectorizer lower cases the words and removes words of less than 2 letters from the \n",
    "vectorizer = TfidfVectorizer(tokenizer=lemma_tokenizer, stop_words=stopWords ,min_df = 10)\n",
    "X_automotive = vectorizer.fit_transform(automotive_data)\n",
    "\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bring', 'bringing', 'broke', 'broken', 'brought', 'bubble', 'buck', 'budget', 'bug', 'building', 'bumper', 'bunch', 'business', 'busy', 'buy', 'buyer', 'buying', 'cadillac', 'california', 'call']\n"
     ]
    }
   ],
   "source": [
    "#I can then print a sample number of the terms\n",
    "print(terms[200:220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 6.93121053,  6.96413067, 13.86814523, ...,  5.24028037,\n",
       "          3.06906265,  1.90300812]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now that I have the document-term matrix, I need to find the most commonly used words throughout the vectorizer of terms\n",
    "freqs = X_automotive.sum(axis=0)\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car = 123.13\n",
      "this = 88.35\n",
      "service = 76.85\n",
      "very = 65.37\n",
      "great = 65.30\n",
      "get = 60.22\n",
      "time = 59.11\n",
      "tire = 56.87\n",
      "work = 55.83\n",
      "back = 53.46\n"
     ]
    }
   ],
   "source": [
    "# sort the indexes of the array by value, and then reverse it\n",
    "sorted_term_indexes = freqs.argsort()\n",
    "sorted_term_indexes = sorted_term_indexes[0, ::-1]\n",
    "# display the top 10 terms\n",
    "for i in range(10):\n",
    "    term_index = sorted_term_indexes[0,i]\n",
    "    print(\"%s = %.2f\" % ( terms[term_index], freqs[0,term_index] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a warning regarding the default values of n_estimators in the RandomForestClassifier changing from 10 in version 0.20 to 100 in version 0.22. This has been supressed as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "classifier_model = RandomForestClassifier()\n",
    "# build a model on the document-term matrix created from the original set of documents\n",
    "classifier_model.fit(X_automotive, automotive_targets)\n",
    "print(classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82587065 0.82587065 0.855      0.805      0.84       0.79\n",
      " 0.805      0.79       0.86432161 0.84422111]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8245284007100178"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can apply 10-fold cross validation to the model above to test performance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(classifier_model, X_automotive, automotive_targets, cv=10, scoring=\"accuracy\")\n",
    "print(scores)\n",
    "\n",
    "scores.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85106383 0.85       0.86746988 0.83261803 0.85020243 0.80672269\n",
      " 0.81702128 0.83050847 0.85943775 0.83116883]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8396213186626669"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(classifier_model, X_automotive, automotive_targets, cv=10, scoring=\"f1\")\n",
    "print(scores)\n",
    "\n",
    "scores.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88709677 0.89166667 0.87301587 0.85087719 0.86324786 0.86290323\n",
      " 0.88617886 0.84955752 0.80291971 0.91588785]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8683351538321858"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(classifier_model, X_automotive, automotive_targets, cv=10, scoring=\"precision\")\n",
    "print(scores)\n",
    "\n",
    "scores.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85245902 0.8852459  0.88429752 0.87603306 0.87603306 0.8677686\n",
      " 0.79338843 0.80165289 0.88429752 0.8677686 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8588944587454275"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(classifier_model, X_automotive, automotive_targets, cv=10, scoring=\"recall\")\n",
    "print(scores)\n",
    "\n",
    "scores.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMENT ON THE RESULTS LISTED ABOVE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
